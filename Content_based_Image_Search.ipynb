{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Create Descriptions**"
      ],
      "metadata": {
        "id": "J0Ate3tVQvRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "t2XUI9ivOj3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caV8ifwmNhjf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import InstructBlipForConditionalGeneration, InstructBlipProcessor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = InstructBlipForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/instructblip-vicuna-7b\",\n",
        "    load_in_4bit = True,\n",
        "    torch_dtype = torch.bfloat16,\n",
        ")"
      ],
      "metadata": {
        "id": "earC12XGU3zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor = InstructBlipProcessor.from_pretrained(\n",
        "    \"Salesforce/instructblip-vicuna-7b\",\n",
        ")"
      ],
      "metadata": {
        "id": "bSpSfXsCQwjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = [\n",
        "    (\"detection-datasets/fashionpedia\", None, \"val\"),\n",
        "    (\"keremberke/nfl-object-detection\", \"mini\", \"test\"),\n",
        "    (\"keremberke/plane-detection\", \"mini\", \"train\"),\n",
        "    (\"Matthijs/snacks\", None, \"validation\"),\n",
        "    (\"rokmr/mini_pets\", None, \"test\"),\n",
        "    (\"keremberke/pokemon-classification\", \"mini\", \"train\"),\n",
        "]"
      ],
      "metadata": {
        "id": "3p1gEGQRTan4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt1 = \"describe this image in full detail. describe each and every aspect of the image so that an artist could re create the image\"\n",
        "prompt2 = \"create an extensive description of this image\""
      ],
      "metadata": {
        "id": "ejtKfcWAbQFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counter = 0\n",
        "for name, config, split in datasets:\n",
        "  d = load_dataset(name, config, split = split)\n",
        "  for idx in range(len(d)):\n",
        "    image = d[idx][\"image\"]\n",
        "    desc = \"\"\n",
        "    for _prompt in [prompt1, prompt2]:\n",
        "      inputs = processor(\n",
        "          images = image,\n",
        "          text = _prompt,\n",
        "          return_rensors = \"pt\"\n",
        "      ).to(model.device, torch.bfloat16)\n",
        "      outputs = model.generate(\n",
        "          **inputs,\n",
        "          do_sample = False,\n",
        "          num_beams = 10,\n",
        "          max_length = 512,\n",
        "          min_length = 16,\n",
        "          top_p = 0.9,\n",
        "          repetition_penalty = 1.5,\n",
        "          temperature = 1,\n",
        "      )\n",
        "      generated_text = processor.batch_decode(\n",
        "          outputs,\n",
        "          skip_special_tokens = True,\n",
        "      )[0].strip()\n",
        "\n",
        "      desc += generated_text + \" \"\n",
        "\n",
        "    desc = desc.strip() #remove \\n \\t\n",
        "    image.save(f\"images/{counter}.jpg\")\n",
        "    print(counter, desc)\n",
        "\n",
        "    with open(\"description.csv\", \"a\") as f:\n",
        "      f.write(f\"{counter}, {desc}\\n\")\n",
        "\n",
        "    counter+=1\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Ij6UaloBVQmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create Embeddings**"
      ],
      "metadata": {
        "id": "oKHerGVXQ0m_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "J1eYE6Z2Rmti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "with open(\"descriptions.csv\", \"r\") as f:\n",
        "  lines = f.readlines()\n",
        "\n",
        "lines = [line.strip().split(\",\") for line in lines]\n",
        "\n",
        "for idx, line in enumerate(lines):\n",
        "  lines[idx] = [line[0], \",\".join(line[1:])]\n",
        "\n",
        "df = pd.Dataframe(lines, columns = [\"id\", \"desc\"])\n",
        "embeddings = model.encode(df[\"desc\"].tolist(), show_progress_bar = True)\n",
        "\n",
        "with open(\"embeddings.pkl\", \"wb\") as f:\n",
        "  pickle.dump(embeddings, f)\n",
        "\n",
        "print(embeddings.shape)\n"
      ],
      "metadata": {
        "id": "toRXCcVJQ3vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **App**"
      ],
      "metadata": {
        "id": "3qstkgAFY74w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import pickle\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTranformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "with open(\"embeddings.pkl\", \"rb\") as f:\n",
        "  embeddings = pickle.load(f)\n",
        "embeddings = embeddings.astype(\"float32\")\n",
        "\n",
        "embedding_size = embeddings.shape[1]\n",
        "n_clusters = 5\n",
        "num_results = 5\n",
        "\n",
        "quantizer = faiss.IndexFlatIP(embedding_size)\n",
        "index = faiss.IndexIVFFlat(\n",
        "    quantizer, embedding_size, n_clusters, faiss.METRIC_INNER_PRODUCT,\n",
        ")\n",
        "\n",
        "index.train(embeddings)\n",
        "index.add(embeddings)\n",
        "\n",
        "def _search(query):\n",
        "  query_embedding = model.encode(query)\n",
        "  query_embedding = np.array(query_embeedding).astype(\"float32\")\n",
        "  query_embedding = query_embedding.reshape(1, -1)\n",
        "  _, indices = index.search(query_embedding, num_results)\n",
        "  images = [f\"images/{i}.jpg\" for i in indices[0]]\n",
        "  return images\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "  query = gr.Textbox(lines = 1, label = \"search query\")\n",
        "  outputs = gr.Gallery(preview = True)\n",
        "  submit = gr.Button(value = \"search\")\n",
        "  submit.click(_search, inputs = query, outputs = outputs)\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "THeXtR3NY6cM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}